{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic-PyTorch_NLP_DeepLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Z2C0_PC0ozE-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Basic Pytoch - Word Embeddings with Deep Learning"
      ]
    },
    {
      "metadata": {
        "id": "K95ZubPv9thc",
        "colab_type": "code",
        "outputId": "2ef18656-9efa-4be0-aace-6663e79139b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# NVIDIA profiling tool for the available GPU\n",
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Tue_Jun_12_23:07:04_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tgn-12uB90vN",
        "colab_type": "code",
        "outputId": "c193faa0-785f-4212-c451-a91edf7be642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 27 06:06:43 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gs3wYPfu_Xcj",
        "colab_type": "code",
        "outputId": "e9c9d891-b383-48cc-c1d2-afb3a73beb23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# CLone my repo that contains the shell file\n",
        "!git clone https://gist.github.com/f7b7c7758a46da49f84bc68b47997d69.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'f7b7c7758a46da49f84bc68b47997d69'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uzPuenG_Z42",
        "colab_type": "code",
        "outputId": "4c6a7ba8-7033-400c-9408-c64078f788bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Move to te directory where the file was donwloaded\n",
        "cd f7b7c7758a46da49f84bc68b47997d69/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/f7b7c7758a46da49f84bc68b47997d69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CN6cqMfd_bdN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Execute the shell script. \n",
        "# NOTE: This takes sometime and it breaks the connection. Better use the steps after and execute them one by one.\n",
        "\n",
        "#!bash pytorch041_cuda92_colab.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9U5I8GX_c-W",
        "colab_type": "code",
        "outputId": "bcea61e4-c655-4965-b2d0-d59ec5c6d1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-27 06:07:15--  https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.162.216\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.162.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?UMU2C4btU2wXIkcr0Cq3vfPXFLxUJoPA0wnw8zgYkoMB4Sz8xo5-VTXhhIIp6r82jor_ePKFseHh3OHMU-ueWQQUsJgV1rI_R-5jQp-EiLsh13kYQIWrb2s0ssvtmM5CYFNCYtr8ulEv_9dUKS9YdTVS15qbUrQvPodLurYx_zQjNuHXuJBZlvzZFDSHB58Ks2cU7duqeCGa86NwxpVCog [following]\n",
            "--2019-01-27 06:07:16--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?UMU2C4btU2wXIkcr0Cq3vfPXFLxUJoPA0wnw8zgYkoMB4Sz8xo5-VTXhhIIp6r82jor_ePKFseHh3OHMU-ueWQQUsJgV1rI_R-5jQp-EiLsh13kYQIWrb2s0ssvtmM5CYFNCYtr8ulEv_9dUKS9YdTVS15qbUrQvPodLurYx_zQjNuHXuJBZlvzZFDSHB58Ks2cU7duqeCGa86NwxpVCog\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.211.70, 2606:2800:21f:3aa:dcf:37b:1ed6:1fb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.211.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1267151038 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.18G   136MB/s    in 9.4s    \n",
            "\n",
            "2019-01-27 06:07:25 (128 MB/s) - ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64’ saved [1267151038/1267151038]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LUuPl2Qb_fTZ",
        "colab_type": "code",
        "outputId": "a6997dfe-be47-429b-dd0a-896531352ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "!dpkg --install cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-2-local.\n",
            "(Reading database ... 110851 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64 ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-2-local (9.2.148-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-2-local (9.2.148-1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cXxjAxhg_jbD",
        "colab_type": "code",
        "outputId": "1166fff9-85b2-44a3-bd42-1b57c367faa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IxmjHrZi_m3Z",
        "colab_type": "code",
        "outputId": "bc181dd8-f553-490a-ddd1-8ddfa2e35e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 file:/var/cuda-repo-9-2-local  InRelease\n",
            "\r            \rIgn:1 file:/var/cuda-repo-9-2-local  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\r                                                                               \rGet:2 file:/var/cuda-repo-9-2-local  Release [574 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [2 Re\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn\r                                                                               \rGet:2 file:/var/cuda-repo-9-2-local  Release [574 B]\n",
            "\r0% [2 Release 0 B/574 B 0%] [Connecting to archive.ubuntu.com (91.189.88.152)] \r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r                                                                               \rGet:3 file:/var/cuda-repo-9-2-local  Release.gpg [819 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r                                                                               \rGet:3 file:/var/cuda-repo-9-2-local  Release.gpg [819 B]\n",
            "\r0% [3 Release.gpg 0 B/819 B 0%] [Connecting to archive.ubuntu.com (91.189.88.15\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [3 Release.gpg gpgv 574 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rGet:4 file:/var/cuda-repo-9-2-local  Packages [18.7 kB]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [4 Packages 0 B/18.7 kB 0%] [Wai\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rGet:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [6 InRelea\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [6 InRelea\r0% [5 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\r                                                                               \rGet:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [5 InRelease gpgv 242 kB] [Waiting for headers] [7 InRelease 2,587 B/88.7 kB\r                                                                               \rIgn:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  InRelease\n",
            "\r0% [5 InRelease gpgv 242 kB] [Waiting for headers] [7 InRelease 14.2 kB/88.7 kB\r                                                                               \rGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
            "Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  Release\n",
            "Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [27.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [310 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [6,955 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [910 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [3,451 B]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [140 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [638 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,666 B]\n",
            "Fetched 2,312 kB in 2s (1,226 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SwAD37gy_n1v",
        "colab_type": "code",
        "outputId": "9187f4ae-a3e4-47bb-cb1f-4189e58f3e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "# NOTE: This might take some time..\n",
        "!apt-get install cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda is already the newest version (9.2.148-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 72 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PU-qIOfQ_ppE",
        "colab_type": "code",
        "outputId": "bde4812e-6e33-4d9e-bb4e-d0cb63ac1462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Check the version of CUDA on the system\n",
        "!cat /usr/local/cuda/version.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Version 9.2.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V4BOpVjF_rWm",
        "colab_type": "code",
        "outputId": "e17cc0fb-d366-4fcb-94fa-08ac08b03bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (512.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 512.6MB 34.3MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58922000 @  0x7f8cb8da52a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.41 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.0.0\n",
            "    Uninstalling torch-1.0.0:\n",
            "      Successfully uninstalled torch-1.0.0\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x56KZSy5_s7F",
        "colab_type": "code",
        "outputId": "5fee7775-1e79-4756-cd7c-f5aaa51a3b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 11.0MB/s \n",
            "\u001b[31mfastai 1.0.41 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1\n",
            "\u001b[0;31;1mWARNING: The following packages were previously imported in this runtime:\n",
            "  [PIL]\n",
            "You must restart the runtime in order to use newly installed versions.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6BPAsobRAAaH",
        "colab_type": "code",
        "outputId": "7ced9ca9-ca7f-4ebc-ee3f-14fdb2ff599c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version: \")\n",
        "print(torch.__version__)\n",
        "print(\"CUDA Version: \")\n",
        "print(torch.version.cuda)\n",
        "print(\"cuDNN version is: \")\n",
        "print(torch.backends.cudnn.version())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: \n",
            "1.0.0\n",
            "CUDA Version: \n",
            "9.0.176\n",
            "cuDNN version is: \n",
            "7401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t6rV3ftZDP0H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fv8sVI4TDREf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Basic examples of NLP with PyTorch**\n"
      ]
    },
    {
      "metadata": {
        "id": "2pHEgVhlAO_H",
        "colab_type": "code",
        "outputId": "20f093ca-22aa-480c-9bed-3353c41f7898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7faaec84f9f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "jORnwK6vDstn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic Word Embedding"
      ]
    },
    {
      "metadata": {
        "id": "cyQbIpsuBIJP",
        "colab_type": "code",
        "outputId": "01e4e444-8ebe-4798-e72a-2ece53169af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1, \"!\": 2}\n",
        "embeds = nn.Embedding(3, 5)  # 3 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Ql3Xe8DHzG8",
        "colab_type": "code",
        "outputId": "95ae13b2-c630-42bc-fe1d-b58fca653867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "cell_type": "code",
      "source": [
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
        "world_embed = embeds(lookup_tensor)\n",
        "token_tensor = torch.tensor([word_to_ix[\"!\"]], dtype=torch.long)\n",
        "token_embed = embeds(token_tensor)\n",
        "array_test = [hello_embed, world_embed, token_embed]\n",
        "print(type(array_test))\n",
        "array_tensor = torch.stack(array_test)\n",
        "sum_token = torch.mean(array_tensor, keepdim=True, dim=0 )\n",
        "print(hello_embed)\n",
        "print(world_embed)\n",
        "print(token_embed)\n",
        "print(token_embed.size())\n",
        "print(array_tensor)\n",
        "print(array_tensor.size())\n",
        "\n",
        "print(sum_token)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "tensor([[-0.8923, -0.0583, -0.1955, -0.9656,  0.4224]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([1, 5])\n",
            "tensor([[[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "\n",
            "        [[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
            "\n",
            "        [[-0.8923, -0.0583, -0.1955, -0.9656,  0.4224]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "torch.Size([3, 1, 5])\n",
            "tensor([[[-0.1324, -0.4380,  0.0826, -0.4573, -0.1975]]],\n",
            "       grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3T2aOvu5D0__",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## N-Gram Language Modeling"
      ]
    },
    {
      "metadata": {
        "id": "Vr47pPkbBKfM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFHk-rt9BQwc",
        "colab_type": "code",
        "outputId": "8989ac4b-d48b-47d4-fed1-2fabaf28fbbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3TE3XTLKBTkI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "meb0mY23BXyC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ls9ovi34Ba2n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DfmYC8IBBc9C",
        "colab_type": "code",
        "outputId": "91ded7be-cb56-4ae9-922f-bd063cc33ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1697
        }
      },
      "cell_type": "code",
      "source": [
        "n_epoch = 100\n",
        "for epoch in range(n_epoch):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    print(\"epoch(%d) loss: %f\"%(epoch, total_loss))\n",
        "    losses.append(total_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch(0) loss: 522.334395\n",
            "epoch(1) loss: 519.845836\n",
            "epoch(2) loss: 517.373542\n",
            "epoch(3) loss: 514.916929\n",
            "epoch(4) loss: 512.475893\n",
            "epoch(5) loss: 510.050383\n",
            "epoch(6) loss: 507.639275\n",
            "epoch(7) loss: 505.242038\n",
            "epoch(8) loss: 502.857359\n",
            "epoch(9) loss: 500.485287\n",
            "epoch(10) loss: 498.124641\n",
            "epoch(11) loss: 495.775747\n",
            "epoch(12) loss: 493.437327\n",
            "epoch(13) loss: 491.108869\n",
            "epoch(14) loss: 488.789346\n",
            "epoch(15) loss: 486.476932\n",
            "epoch(16) loss: 484.171072\n",
            "epoch(17) loss: 481.870793\n",
            "epoch(18) loss: 479.576999\n",
            "epoch(19) loss: 477.287404\n",
            "epoch(20) loss: 475.003558\n",
            "epoch(21) loss: 472.724062\n",
            "epoch(22) loss: 470.447245\n",
            "epoch(23) loss: 468.173134\n",
            "epoch(24) loss: 465.900571\n",
            "epoch(25) loss: 463.631922\n",
            "epoch(26) loss: 461.366170\n",
            "epoch(27) loss: 459.101134\n",
            "epoch(28) loss: 456.838301\n",
            "epoch(29) loss: 454.576596\n",
            "epoch(30) loss: 452.316368\n",
            "epoch(31) loss: 450.055780\n",
            "epoch(32) loss: 447.796130\n",
            "epoch(33) loss: 445.535799\n",
            "epoch(34) loss: 443.274056\n",
            "epoch(35) loss: 441.012017\n",
            "epoch(36) loss: 438.746049\n",
            "epoch(37) loss: 436.477675\n",
            "epoch(38) loss: 434.206952\n",
            "epoch(39) loss: 431.933593\n",
            "epoch(40) loss: 429.658120\n",
            "epoch(41) loss: 427.377511\n",
            "epoch(42) loss: 425.092522\n",
            "epoch(43) loss: 422.803182\n",
            "epoch(44) loss: 420.507910\n",
            "epoch(45) loss: 418.207856\n",
            "epoch(46) loss: 415.900675\n",
            "epoch(47) loss: 413.586367\n",
            "epoch(48) loss: 411.265224\n",
            "epoch(49) loss: 408.938994\n",
            "epoch(50) loss: 406.603368\n",
            "epoch(51) loss: 404.258723\n",
            "epoch(52) loss: 401.907699\n",
            "epoch(53) loss: 399.545069\n",
            "epoch(54) loss: 397.175821\n",
            "epoch(55) loss: 394.795789\n",
            "epoch(56) loss: 392.405828\n",
            "epoch(57) loss: 390.005872\n",
            "epoch(58) loss: 387.596501\n",
            "epoch(59) loss: 385.175814\n",
            "epoch(60) loss: 382.745621\n",
            "epoch(61) loss: 380.306051\n",
            "epoch(62) loss: 377.854907\n",
            "epoch(63) loss: 375.396460\n",
            "epoch(64) loss: 372.925204\n",
            "epoch(65) loss: 370.442887\n",
            "epoch(66) loss: 367.948932\n",
            "epoch(67) loss: 365.446400\n",
            "epoch(68) loss: 362.932232\n",
            "epoch(69) loss: 360.408104\n",
            "epoch(70) loss: 357.873713\n",
            "epoch(71) loss: 355.325952\n",
            "epoch(72) loss: 352.770460\n",
            "epoch(73) loss: 350.203323\n",
            "epoch(74) loss: 347.626308\n",
            "epoch(75) loss: 345.038127\n",
            "epoch(76) loss: 342.440068\n",
            "epoch(77) loss: 339.833057\n",
            "epoch(78) loss: 337.214266\n",
            "epoch(79) loss: 334.587627\n",
            "epoch(80) loss: 331.949251\n",
            "epoch(81) loss: 329.301976\n",
            "epoch(82) loss: 326.647224\n",
            "epoch(83) loss: 323.982056\n",
            "epoch(84) loss: 321.309813\n",
            "epoch(85) loss: 318.628440\n",
            "epoch(86) loss: 315.942845\n",
            "epoch(87) loss: 313.249826\n",
            "epoch(88) loss: 310.549311\n",
            "epoch(89) loss: 307.843931\n",
            "epoch(90) loss: 305.133422\n",
            "epoch(91) loss: 302.416046\n",
            "epoch(92) loss: 299.693463\n",
            "epoch(93) loss: 296.965390\n",
            "epoch(94) loss: 294.232767\n",
            "epoch(95) loss: 291.497456\n",
            "epoch(96) loss: 288.757165\n",
            "epoch(97) loss: 286.015532\n",
            "epoch(98) loss: 283.270244\n",
            "epoch(99) loss: 280.523211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2A-llJDDEFxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag of Words"
      ]
    },
    {
      "metadata": {
        "id": "_YaWIxGpEKz_",
        "colab_type": "code",
        "outputId": "0f7ff787-ff3c-4ade-8c6f-19275a8e9139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(\"Vocab size: %d\"%vocab_size)\n",
        "print(data[0])\n",
        "print(data[1])\n",
        "print(data[2])\n",
        "print(data[3])\n",
        "print(data[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 49\n",
            "(['We', 'are', 'to', 'study'], 'about')\n",
            "(['are', 'about', 'study', 'the'], 'to')\n",
            "(['about', 'to', 'the', 'idea'], 'study')\n",
            "(['to', 'study', 'idea', 'of'], 'the')\n",
            "(['study', 'the', 'of', 'a'], 'idea')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gLF7AFuUiYPR",
        "colab_type": "code",
        "outputId": "38d03581-f911-4523-b75d-649d7998f8f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'conjure': 0, 'effect,': 1, 'to': 2, 'things': 3, 'pattern': 4, 'idea': 5, 'program.': 6, 'they': 7, 'are': 8, 'spells.': 9, 'our': 10, 'data.': 11, 'People': 12, 'The': 13, 'direct': 14, 'by': 15, 'evolution': 16, 'computational': 17, 'that': 18, 'programs': 19, 'spirits': 20, 'directed': 21, 'with': 22, 'study': 23, 'of': 24, 'process': 25, 'the': 26, 'other': 27, 'inhabit': 28, 'is': 29, 'rules': 30, 'beings': 31, 'computer': 32, 'abstract': 33, 'computers.': 34, 'In': 35, 'As': 36, 'a': 37, 'processes': 38, 'process.': 39, 'create': 40, 'processes.': 41, 'Computational': 42, 'we': 43, 'called': 44, 'manipulate': 45, 'evolve,': 46, 'about': 47, 'We': 48}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PdKUkENEiplC",
        "colab_type": "code",
        "outputId": "d10e597b-294e-4bd7-fdf5-37e7628dfeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea'), (['the', 'idea', 'a', 'computational'], 'of'), (['idea', 'of', 'computational', 'process.'], 'a'), (['of', 'a', 'process.', 'Computational'], 'computational'), (['a', 'computational', 'Computational', 'processes'], 'process.'), (['computational', 'process.', 'processes', 'are'], 'Computational'), (['process.', 'Computational', 'are', 'abstract'], 'processes'), (['Computational', 'processes', 'abstract', 'beings'], 'are'), (['processes', 'are', 'beings', 'that'], 'abstract'), (['are', 'abstract', 'that', 'inhabit'], 'beings'), (['abstract', 'beings', 'inhabit', 'computers.'], 'that'), (['beings', 'that', 'computers.', 'As'], 'inhabit'), (['that', 'inhabit', 'As', 'they'], 'computers.'), (['inhabit', 'computers.', 'they', 'evolve,'], 'As'), (['computers.', 'As', 'evolve,', 'processes'], 'they'), (['As', 'they', 'processes', 'manipulate'], 'evolve,'), (['they', 'evolve,', 'manipulate', 'other'], 'processes'), (['evolve,', 'processes', 'other', 'abstract'], 'manipulate'), (['processes', 'manipulate', 'abstract', 'things'], 'other'), (['manipulate', 'other', 'things', 'called'], 'abstract'), (['other', 'abstract', 'called', 'data.'], 'things'), (['abstract', 'things', 'data.', 'The'], 'called'), (['things', 'called', 'The', 'evolution'], 'data.'), (['called', 'data.', 'evolution', 'of'], 'The'), (['data.', 'The', 'of', 'a'], 'evolution'), (['The', 'evolution', 'a', 'process'], 'of'), (['evolution', 'of', 'process', 'is'], 'a'), (['of', 'a', 'is', 'directed'], 'process'), (['a', 'process', 'directed', 'by'], 'is'), (['process', 'is', 'by', 'a'], 'directed'), (['is', 'directed', 'a', 'pattern'], 'by'), (['directed', 'by', 'pattern', 'of'], 'a'), (['by', 'a', 'of', 'rules'], 'pattern'), (['a', 'pattern', 'rules', 'called'], 'of'), (['pattern', 'of', 'called', 'a'], 'rules'), (['of', 'rules', 'a', 'program.'], 'called'), (['rules', 'called', 'program.', 'People'], 'a'), (['called', 'a', 'People', 'create'], 'program.'), (['a', 'program.', 'create', 'programs'], 'People'), (['program.', 'People', 'programs', 'to'], 'create'), (['People', 'create', 'to', 'direct'], 'programs'), (['create', 'programs', 'direct', 'processes.'], 'to'), (['programs', 'to', 'processes.', 'In'], 'direct'), (['to', 'direct', 'In', 'effect,'], 'processes.'), (['direct', 'processes.', 'effect,', 'we'], 'In'), (['processes.', 'In', 'we', 'conjure'], 'effect,'), (['In', 'effect,', 'conjure', 'the'], 'we'), (['effect,', 'we', 'the', 'spirits'], 'conjure'), (['we', 'conjure', 'spirits', 'of'], 'the'), (['conjure', 'the', 'of', 'the'], 'spirits'), (['the', 'spirits', 'the', 'computer'], 'of'), (['spirits', 'of', 'computer', 'with'], 'the'), (['of', 'the', 'with', 'our'], 'computer'), (['the', 'computer', 'our', 'spells.'], 'with')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OoaoDFBCENpB",
        "colab_type": "code",
        "outputId": "73f06cc1-dddc-4f77-e1df-0adf621b711f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "  idxs = [word_to_ix[w] for w in context]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([48,  8,  2, 23])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "cBBI4d9-iCnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_train = [(make_context_vector(x_word, word_to_ix), word_to_ix[y_word]) \n",
        "              for x_word, y_word in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7LIs715GD_c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 110"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s78hjNuXGDjd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Ei8c3rRREyrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.context_size = context_size\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, vocab_size)\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((self.context_size*2, embedding_dim))\n",
        "    hidden1_mean = torch.mean(embeds, keepdim=True, dim=0)\n",
        "    z1 = self.linear1(hidden1_mean)\n",
        "    z2 = self.linear2(z1)\n",
        "    y_pred = F.log_softmax(z2, dim=1)\n",
        "    return y_pred\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHGB9m2Xfx6W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "model = CBOW(vocab_size, embedding_dim, CONTEXT_SIZE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EunKHXudg8kR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epoch = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0FLZqWP3hNZY",
        "colab_type": "code",
        "outputId": "9d848525-ebda-453b-f390-1335105096d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1697
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epoch):\n",
        "  total_loss = 0\n",
        "  for xs_ids, y_id in data_train:\n",
        "    model.zero_grad()\n",
        "    y_pred = model(xs_ids)\n",
        "    loss = loss_function(y_pred, torch.tensor([y_id], dtype=torch.long))\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "  print(\"epoch (%s/%s): %f\"%(epoch, n_epoch, total_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch (0/100): 224.768022\n",
            "epoch (1/100): 185.773384\n",
            "epoch (2/100): 150.560980\n",
            "epoch (3/100): 115.242117\n",
            "epoch (4/100): 83.091576\n",
            "epoch (5/100): 56.674554\n",
            "epoch (6/100): 37.572915\n",
            "epoch (7/100): 25.064697\n",
            "epoch (8/100): 17.234778\n",
            "epoch (9/100): 12.344531\n",
            "epoch (10/100): 9.208761\n",
            "epoch (11/100): 7.116354\n",
            "epoch (12/100): 5.661807\n",
            "epoch (13/100): 4.612602\n",
            "epoch (14/100): 3.831340\n",
            "epoch (15/100): 3.233698\n",
            "epoch (16/100): 2.765981\n",
            "epoch (17/100): 2.392781\n",
            "epoch (18/100): 2.089996\n",
            "epoch (19/100): 1.840802\n",
            "epoch (20/100): 1.633118\n",
            "epoch (21/100): 1.458138\n",
            "epoch (22/100): 1.309261\n",
            "epoch (23/100): 1.181510\n",
            "epoch (24/100): 1.071034\n",
            "epoch (25/100): 0.974828\n",
            "epoch (26/100): 0.890526\n",
            "epoch (27/100): 0.816235\n",
            "epoch (28/100): 0.750426\n",
            "epoch (29/100): 0.691846\n",
            "epoch (30/100): 0.639482\n",
            "epoch (31/100): 0.592476\n",
            "epoch (32/100): 0.550126\n",
            "epoch (33/100): 0.511846\n",
            "epoch (34/100): 0.477118\n",
            "epoch (35/100): 0.445531\n",
            "epoch (36/100): 0.416724\n",
            "epoch (37/100): 0.390379\n",
            "epoch (38/100): 0.366219\n",
            "epoch (39/100): 0.344018\n",
            "epoch (40/100): 0.323568\n",
            "epoch (41/100): 0.304705\n",
            "epoch (42/100): 0.287262\n",
            "epoch (43/100): 0.271105\n",
            "epoch (44/100): 0.256120\n",
            "epoch (45/100): 0.242193\n",
            "epoch (46/100): 0.229224\n",
            "epoch (47/100): 0.217136\n",
            "epoch (48/100): 0.205853\n",
            "epoch (49/100): 0.195310\n",
            "epoch (50/100): 0.185440\n",
            "epoch (51/100): 0.176197\n",
            "epoch (52/100): 0.167521\n",
            "epoch (53/100): 0.159374\n",
            "epoch (54/100): 0.151713\n",
            "epoch (55/100): 0.144514\n",
            "epoch (56/100): 0.137723\n",
            "epoch (57/100): 0.131324\n",
            "epoch (58/100): 0.125297\n",
            "epoch (59/100): 0.119584\n",
            "epoch (60/100): 0.114201\n",
            "epoch (61/100): 0.109102\n",
            "epoch (62/100): 0.104273\n",
            "epoch (63/100): 0.099705\n",
            "epoch (64/100): 0.095371\n",
            "epoch (65/100): 0.091261\n",
            "epoch (66/100): 0.087356\n",
            "epoch (67/100): 0.083653\n",
            "epoch (68/100): 0.080132\n",
            "epoch (69/100): 0.076789\n",
            "epoch (70/100): 0.073601\n",
            "epoch (71/100): 0.070573\n",
            "epoch (72/100): 0.067684\n",
            "epoch (73/100): 0.064937\n",
            "epoch (74/100): 0.062309\n",
            "epoch (75/100): 0.059811\n",
            "epoch (76/100): 0.057422\n",
            "epoch (77/100): 0.055146\n",
            "epoch (78/100): 0.052973\n",
            "epoch (79/100): 0.050897\n",
            "epoch (80/100): 0.048908\n",
            "epoch (81/100): 0.047010\n",
            "epoch (82/100): 0.045202\n",
            "epoch (83/100): 0.043463\n",
            "epoch (84/100): 0.041799\n",
            "epoch (85/100): 0.040204\n",
            "epoch (86/100): 0.038686\n",
            "epoch (87/100): 0.037220\n",
            "epoch (88/100): 0.035828\n",
            "epoch (89/100): 0.034487\n",
            "epoch (90/100): 0.033204\n",
            "epoch (91/100): 0.031966\n",
            "epoch (92/100): 0.030787\n",
            "epoch (93/100): 0.029651\n",
            "epoch (94/100): 0.028567\n",
            "epoch (95/100): 0.027526\n",
            "epoch (96/100): 0.026522\n",
            "epoch (97/100): 0.025559\n",
            "epoch (98/100): 0.024634\n",
            "epoch (99/100): 0.023744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EWcovl57DANo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **References** \n",
        "* Word Embeddings: Encoding Lexical Semantics, Part I https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py \n",
        "* TORCH.OPTIM https://pytorch.org/docs/stable/optim.html\n"
      ]
    }
  ]
}